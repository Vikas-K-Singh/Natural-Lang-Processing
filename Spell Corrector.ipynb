{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Words already Exists in dictionary from Text5 2443\n",
      "Number of Words that Not Exists in dictionary from Text5 2016\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Build a simple weighted minimum edit distance spell corrector using\n",
    "Dynamic Programming and use it for text5  in nltk.book.\n",
    "Evaluate your spell corrector. Determine the number of words\n",
    "from text5  that are present in the dictionary before\n",
    "and after spell correction. Top 100 Words Spell Correction\n",
    "Created on Tue Mar 11 10:37:28 2019\n",
    "@author: Vikas Singh\n",
    "\"\"\"\n",
    "from pandas import DataFrame, Series\n",
    "from nltk.corpus import stopwords \n",
    "import re\n",
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) \n",
    "FltrdText = [w for w in text5 if not w in stop_words] \n",
    "FltrdText = [w.lower() for w in FltrdText]\n",
    "FltrdText = sorted(set(FltrdText))\n",
    "a=[]\n",
    "for w in FltrdText:\n",
    "    if(re.match(('^\\w+$'),w)):\n",
    "        a.append(w)\n",
    "processed=[]\n",
    "for w in a:\n",
    "    x=re.sub(r'\\w*\\d\\w*', '', w).strip()\n",
    "    if(len(x)>2):\n",
    "        processed.append(x)\n",
    "processed= sorted(set(processed))\n",
    "\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "OrgInDic=[]\n",
    "for i in range(len(processed)):\n",
    "    OrgInDic.append(processed[i] in english_vocab)\n",
    "print(\"Number of Words already Exists in dictionary from Text5\",sum(OrgInDic))\n",
    "print(\"Number of Words that Not Exists in dictionary from Text5\",len(processed)-sum(OrgInDic))\n",
    "df1=pd.DataFrame(data=[OrgInDic,processed])\n",
    "df1=df1.transpose()\n",
    "df1.columns=['OrgInDic','Words']\n",
    "\n",
    "# Increasing efficiency of our code by adding memory using decorator\n",
    "#we add some \"memory\" to our recursive Levenshtein function by adding a dictionary memory \n",
    "def call_counter(func):\n",
    "    def helper(*args, **kwargs):\n",
    "        helper.calls += 1\n",
    "        return func(*args, **kwargs)\n",
    "    helper.calls = 0\n",
    "    helper.__name__= func.__name__\n",
    "    return helper\n",
    "def memoize(func):\n",
    "    mem = {}\n",
    "    def memoizer(*args, **kwargs):\n",
    "        key = str(args) + str(kwargs)\n",
    "        if key not in mem:\n",
    "            mem[key] = func(*args, **kwargs)\n",
    "        return mem[key]\n",
    "    return memoizer\n",
    "@call_counter\n",
    "@memoize \n",
    "\n",
    "####Levenshtein Distance Calculator\n",
    "# assumption: Weight of the costs for substitions are twice as high as inserts and delets\n",
    "def levenshtein(s, t, costs=(1, 1, 2)):\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    deletes, inserts, substitutes = costs\n",
    "    \n",
    "    dist = [[0 for x in range(cols)] for x in range(rows)]\n",
    "    # source prefixes can be transformed into empty strings \n",
    "    # by deletions:\n",
    "    for row in range(1, rows):\n",
    "        dist[row][0] = row * deletes\n",
    "    # target prefixes can be created from an empty source string\n",
    "    # by inserting the characters\n",
    "    for col in range(1, cols):\n",
    "        dist[0][col] = col * inserts\n",
    "        \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = substitutes\n",
    "            dist[row][col] = min(dist[row-1][col] + deletes,\n",
    "                                 dist[row][col-1] + inserts,\n",
    "                                 dist[row-1][col-1] + cost) # substitution\n",
    "    return dist[row][col]\n",
    "\n",
    "spell_cor=df1[df1.OrgInDic != True]\n",
    "ed=[]\n",
    "wor=[]\n",
    "i=0\n",
    "lst=list(english_vocab)\n",
    "#Top 100 Words Correction\n",
    "for w in spell_cor.Words:   \n",
    "    for cw in english_vocab:\n",
    "        ed.append(levenshtein(cw, w))   \n",
    "    wor.append(lst[np.argmin(ed)])\n",
    "    ed=[]\n",
    "    i=i+1\n",
    "    if i==100:\n",
    "        break;\n",
    "spelldf=pd.DataFrame(wor) \n",
    "spelldf = pd.concat([spell_cor.Words[0:100], spelldf], axis=1)\n",
    "#spelldf = pd.concat([spell_cor.Words, spelldf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Words SpellCorrected\n",
      "0   aaaaaaaaaaaaaaaaa             aa\n",
      "1             aaahhhh             aa\n",
      "2           abortions       abortion\n",
      "3                abou            abo\n",
      "4            abourted        aborted\n",
      "5                 abs             ab\n",
      "6                 ack           jack\n",
      "7               acros         acrose\n",
      "8             actualy         actual\n",
      "9               adams           adam\n",
      "10               adds            add\n",
      "11            adduser          adder\n",
      "12            adjusts         adjust\n",
      "13             adoted        adopted\n",
      "14         adreniline     adrenaline\n",
      "15                ads             as\n",
      "16             adults          adult\n",
      "17                afe             fe\n",
      "18            affairs         affair\n",
      "19             affari       saffarid\n",
      "20            affects         affect\n",
      "21                afk             ak\n",
      "22             agaibn          again\n",
      "23               ages            age\n",
      "24         aggravated      aggravate\n",
      "25   agurlwithbigguns      lithobius\n",
      "26               ahah            aha\n",
      "27             ahahah            aha\n",
      "28              ahahh            aha\n",
      "29             ahahha            aha\n",
      "..                ...            ...\n",
      "70          apoligize      apologize\n",
      "71        appearently     apparently\n",
      "72            appears         appear\n",
      "73          applaudes        applaud\n",
      "74           appleton      applejohn\n",
      "75         appologise      apologize\n",
      "76         appologize      apologize\n",
      "77           aqwesome        awesome\n",
      "78           arggghhh          bargh\n",
      "79               argh          bargh\n",
      "80          armtnpeat          armet\n",
      "81           arrested     unarrested\n",
      "82            arrived         arrive\n",
      "83            arround         around\n",
      "84              asked         masked\n",
      "85              askin         gaskin\n",
      "86             asking        gasking\n",
      "87               asks            ass\n",
      "88                asl             as\n",
      "89             asnwer         answer\n",
      "90              asses         assets\n",
      "91            asshole          shole\n",
      "92           assholes          soles\n",
      "93               asss            ass\n",
      "94            assumes         assume\n",
      "95             aterry          terry\n",
      "96                atl           atle\n",
      "97          attempted        attempt\n",
      "98          attracted        attract\n",
      "99            aussies         aussie\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "spelldf1=pd.Series(spell_cor.Words[0:100].tolist())\n",
    "spelldf2=pd.Series(wor) \n",
    "\n",
    "results = pd.concat([spelldf1, spelldf2], ignore_index=True, axis=1)\n",
    "results.columns=['Words', 'SpellCorrected']\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
